{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install python packages**"
      ],
      "metadata": {
        "id": "Y14fFewz0R9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRSTD2K30TE4",
        "outputId": "7d828bc8-25dd-4268-9d2a-768e0b274041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers faiss-cpu numpy biopython"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **seq_vdb.py**"
      ],
      "metadata": {
        "id": "uR5qOY2v0cKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, BertConfig\n",
        "from Bio import SeqIO\n",
        "\n",
        "class VectorDatabase:\n",
        "    def __init__(self, model_name='zhihan1996/DNABERT-2-117M', storage_path='vector_db/'):\n",
        "        # Initialize the tokenizer and model for DNABERT\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True, config=BertConfig.from_pretrained(model_name))\n",
        "        self.storage_path = storage_path\n",
        "        self.embeddings = None\n",
        "        self.metadata = []\n",
        "        self.sequences = []  # Store sequences for retrieval\n",
        "        self.index = None\n",
        "\n",
        "        # Create storage directory if it does not exist\n",
        "        os.makedirs(self.storage_path, exist_ok=True)\n",
        "\n",
        "        # Try to load existing database if available\n",
        "        self.load()\n",
        "\n",
        "    def _encode(self, sequence):\n",
        "        \"\"\"Encode a DNA sequence into an embedding.\"\"\"\n",
        "        inputs = self.tokenizer(sequence, return_tensors='pt')[\"input_ids\"]\n",
        "        with torch.no_grad():\n",
        "            hidden_states = self.model(inputs)[0]  # [1, sequence_length, 768]\n",
        "\n",
        "        # Compute embeddings with mean pooling\n",
        "        embedding_mean = torch.mean(hidden_states[0], dim=0).numpy()\n",
        "        return embedding_mean\n",
        "\n",
        "    def create(self, file_path, metadata=None):\n",
        "        \"\"\"Create embeddings for the DNA sequences in a FASTA file and add them to the database.\"\"\"\n",
        "        sequences = [str(record.seq) for record in SeqIO.parse(file_path, 'fasta')]\n",
        "\n",
        "        embeddings = [self._encode(seq) for seq in sequences]\n",
        "        self.metadata.extend([metadata] * len(sequences))\n",
        "        self.sequences.extend(sequences)  # Update self.sequences with the new sequences\n",
        "\n",
        "        for embedding in embeddings:\n",
        "            self._add_embedding(embedding)\n",
        "\n",
        "        self._save()  # Automatically save changes\n",
        "        print(f\"Entries created for file: {file_path}\")\n",
        "\n",
        "    def _add_embedding(self, embedding):\n",
        "        \"\"\"Add an embedding to the FAISS index.\"\"\"\n",
        "        if self.embeddings is None:\n",
        "            self.embeddings = np.array([embedding])\n",
        "            d = embedding.shape[0]\n",
        "            self.index = faiss.IndexFlatL2(d)\n",
        "        else:\n",
        "            self.embeddings = np.vstack((self.embeddings, embedding))\n",
        "\n",
        "        self.index.add(np.array([embedding]))\n",
        "\n",
        "    def read(self, query_file_path, top_k=5):\n",
        "        \"\"\"Search for similar entries in the database using the DNA content of a FASTA file.\"\"\"\n",
        "        if self.index is None:\n",
        "            print(\"No index found. Please load the database.\")\n",
        "            return\n",
        "\n",
        "        query_sequences = [str(record.seq) for record in SeqIO.parse(query_file_path, 'fasta')]\n",
        "\n",
        "        # Iterate over each query sequence\n",
        "        for query_dna in query_sequences:\n",
        "            query_embedding = self._encode(query_dna)\n",
        "            distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "            # Collect results for each sequence\n",
        "            results = []\n",
        "            for j, i in enumerate(indices[0]):\n",
        "                metadata = self.metadata[i]\n",
        "                sequence = self.sequences[i]\n",
        "                similarity_score = 1 - distances[0][j]\n",
        "                results.append((metadata, sequence, similarity_score))\n",
        "\n",
        "            # Print results for each query sequence\n",
        "            print(f\"Query Sequence: {query_dna}\")\n",
        "            if results:\n",
        "                for metadata, sequence, similarity_score in results:\n",
        "                    print(f\"Metadata: {metadata}, Similar Sequence: {sequence}, Similarity Score: {similarity_score:.2f}\")\n",
        "            else:\n",
        "                print(\"No similar sequences found.\")\n",
        "            print()  # Add an empty line between results of different query sequences\n",
        "\n",
        "    def update(self, text_id, new_file_path, new_metadata=None):\n",
        "        \"\"\"Update an existing entry by inserting sequences from a new FASTA file at the specified position.\"\"\"\n",
        "        if text_id >= len(self.metadata):\n",
        "            print(\"Invalid text ID.\")\n",
        "            return\n",
        "\n",
        "        # Read new sequences from the FASTA file\n",
        "        new_sequences = [str(record.seq) for record in SeqIO.parse(new_file_path, 'fasta')]\n",
        "\n",
        "        if not new_sequences:\n",
        "            print(\"No sequences found in the provided FASTA file.\")\n",
        "            return\n",
        "\n",
        "        # Insert new sequences starting from the specified text_id\n",
        "        self.sequences[text_id:text_id] = new_sequences  # Insert at the specified text_id\n",
        "        self.metadata[text_id:text_id] = [new_metadata or {}] * len(new_sequences)  # Insert metadata for each new sequence\n",
        "\n",
        "        # Update embeddings for new sequences\n",
        "        for seq in new_sequences:\n",
        "            new_embedding = self._encode(seq)\n",
        "            self.embeddings = np.insert(self.embeddings, text_id, new_embedding, axis=0)\n",
        "            text_id += 1  # Increment text_id for the next sequence\n",
        "\n",
        "        # Rebuild the FAISS index\n",
        "        self._rebuild_index()\n",
        "        self._save()  # Automatically save changes\n",
        "        print(f\"Inserted {len(new_sequences)} sequences starting at position {text_id - len(new_sequences)}.\")\n",
        "\n",
        "\n",
        "    def delete(self, text_id):\n",
        "        \"\"\"Delete all entries associated with the metadata of the given ID.\"\"\"\n",
        "        if text_id >= len(self.metadata):\n",
        "            print(\"Invalid text ID.\")\n",
        "            return\n",
        "\n",
        "        metadata_to_delete = self.metadata[text_id]\n",
        "\n",
        "        # Find indices of all entries with the same metadata\n",
        "        indices_to_delete = [i for i, meta in enumerate(self.metadata) if meta == metadata_to_delete]\n",
        "\n",
        "        # Remove all sequences and embeddings associated with the metadata\n",
        "        self.embeddings = np.delete(self.embeddings, indices_to_delete, axis=0)\n",
        "        self.metadata = [meta for i, meta in enumerate(self.metadata) if i not in indices_to_delete]\n",
        "        self.sequences = [seq for i, seq in enumerate(self.sequences) if i not in indices_to_delete]\n",
        "\n",
        "        # Rebuild the FAISS index\n",
        "        self._rebuild_index()\n",
        "        self._save()  # Automatically save changes\n",
        "        print(f\"All entries associated with metadata '{metadata_to_delete}' deleted.\")\n",
        "\n",
        "    def _rebuild_index(self):\n",
        "        \"\"\"Rebuild the FAISS index after an update or delete operation.\"\"\"\n",
        "        if self.embeddings is not None and len(self.embeddings) > 0:\n",
        "            d = self.embeddings.shape[1]\n",
        "            self.index = faiss.IndexFlatL2(d)\n",
        "            self.index.add(self.embeddings)\n",
        "        else:\n",
        "            self.index = None\n",
        "\n",
        "    def _save(self):\n",
        "        \"\"\"Save embeddings, metadata, and index to disk.\"\"\"\n",
        "        if self.embeddings is not None:\n",
        "            np.save(os.path.join(self.storage_path, 'embeddings.npy'), self.embeddings)\n",
        "        with open(os.path.join(self.storage_path, 'metadata.json'), 'w') as f:\n",
        "            json.dump(self.metadata, f)\n",
        "        if self.index is not None:\n",
        "            faiss.write_index(self.index, os.path.join(self.storage_path, 'index.faiss'))\n",
        "        with open(os.path.join(self.storage_path, 'sequences.json'), 'w') as f:\n",
        "            json.dump(self.sequences, f)\n",
        "        print(\"Database saved to disk.\")\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Load embeddings, metadata, and index from disk.\"\"\"\n",
        "        embeddings_path = os.path.join(self.storage_path, 'embeddings.npy')\n",
        "        metadata_path = os.path.join(self.storage_path, 'metadata.json')\n",
        "        index_path = os.path.join(self.storage_path, 'index.faiss')\n",
        "        sequences_path = os.path.join(self.storage_path, 'sequences.json')\n",
        "\n",
        "        if os.path.exists(embeddings_path):\n",
        "            self.embeddings = np.load(embeddings_path)\n",
        "        else:\n",
        "            self.embeddings = None\n",
        "\n",
        "        if os.path.exists(metadata_path):\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                self.metadata = json.load(f)\n",
        "        else:\n",
        "            self.metadata = []\n",
        "\n",
        "        if os.path.exists(sequences_path):\n",
        "            with open(sequences_path, 'r') as f:\n",
        "                self.sequences = json.load(f)\n",
        "        else:\n",
        "            self.sequences = []\n",
        "\n",
        "        if os.path.exists(index_path):\n",
        "            self.index = faiss.read_index(index_path)\n",
        "        else:\n",
        "            self.index = None\n",
        "\n",
        "        print(\"Database loaded from disk.\")\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Vector Database CLI')\n",
        "    subparsers = parser.add_subparsers(dest='command')\n",
        "\n",
        "    # Subparser for 'create'\n",
        "    parser_create = subparsers.add_parser('create', help='Create new entries from a FASTA file')\n",
        "    parser_create.add_argument('file_path', type=str, help='Path to the FASTA file')\n",
        "    parser_create.add_argument('--metadata', type=json.loads, default='{}', help='Metadata as JSON')\n",
        "\n",
        "    # Subparser for 'read'\n",
        "    parser_read = subparsers.add_parser('read', help='Read/search entries using a query FASTA file')\n",
        "    parser_read.add_argument('query_file_path', type=str, help='Path to the query FASTA file')\n",
        "    parser_read.add_argument('--top_k', type=int, default=5, help='Number of top results to return')\n",
        "\n",
        "    # Subparser for 'update'\n",
        "    parser_update = subparsers.add_parser('update', help='Update an existing entry using a new FASTA file')\n",
        "    parser_update.add_argument('text_id', type=int, help='ID of the text to update')\n",
        "    parser_update.add_argument('new_file_path', type=str, help='Path to the new FASTA file')\n",
        "    parser_update.add_argument('--new_metadata', type=json.loads, default='{}', help='Updated metadata as JSON')\n",
        "\n",
        "    # Subparser for 'delete'\n",
        "    parser_delete = subparsers.add_parser('delete', help='Delete an entry')\n",
        "    parser_delete.add_argument('text_id', type=int, help='ID of the entry to delete')\n",
        "\n",
        "    # Subparser for 'load'\n",
        "    parser_load = subparsers.add_parser('load', help='Load the database from disk')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    db = VectorDatabase()\n",
        "\n",
        "    if args.command == 'create':\n",
        "        db.create(args.file_path, args.metadata)\n",
        "    elif args.command == 'read':\n",
        "        db.read(args.query_file_path, args.top_k)\n",
        "    elif args.command == 'update':\n",
        "        db.update(args.text_id, args.new_file_path, args.new_metadata)\n",
        "    elif args.command == 'delete':\n",
        "        db.delete(args.text_id)\n",
        "    elif args.command == 'load':\n",
        "        db.load()\n",
        "    else:\n",
        "        print(\"Invalid command. Use 'create', 'read', 'update', 'delete', or 'load'.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "6NnchPDuuqIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create database**"
      ],
      "metadata": {
        "id": "275yJR5w0iYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python seq_vdb.py create /content/cancer.fasta --metadata '{\"Topic\": \"Breast cancer\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFiUYq_20vPU",
        "outputId": "28f72ea0-4c72-43e5-87eb-a3c155d7b19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n",
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Database loaded from disk.\n",
            "Database saved to disk.\n",
            "Entries created for file: /content/cancer.fasta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python seq_vdb.py create /content/non_coding.fasta --metadata '{\"Topic\": \"Non coding variant\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aldA2ePxZJMU",
        "outputId": "cd6d036c-82d9-477d-802b-e555daacec63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n",
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Database loaded from disk.\n",
            "Database saved to disk.\n",
            "Entries created for file: /content/non_coding.fasta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python seq_vdb.py create /content/query.fasta --metadata '{\"Topic\": \"Query\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNvik9gXjsv1",
        "outputId": "e7920979-e54d-4682-b232-1ad0fce558dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n",
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Database loaded from disk.\n",
            "Database saved to disk.\n",
            "Entries created for file: /content/query.fasta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Delete entry**"
      ],
      "metadata": {
        "id": "gBeSLcNT0-MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python seq_vdb.py delete 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGPDwca0aJJC",
        "outputId": "327d3e03-222a-4852-d179-964682b874f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n",
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Database loaded from disk.\n",
            "Database saved to disk.\n",
            "All entries associated with metadata '{'Topic': 'query'}' deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read database or search a query**"
      ],
      "metadata": {
        "id": "CkJWj7qP0yC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python seq_vdb.py read /content/query.fasta --top_k 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsiYBaEiaXHC",
        "outputId": "b3895dbb-6a32-4a95-bfea-f8c723c7739f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n",
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Database loaded from disk.\n",
            "Query Sequence: CTTGCCGCAGCACACTCTCTAATCGAGCTGTAACACTCGCCACTGCCGTCCACGGCTTCATTCCTTGAAGCCGTGAGACCACGAACCCTTCGATTGAGAAGAACCTTCGGTCGGAAGAAGACTTCTCGTCTCAGTAACAATAACAAGAAAAACAGCCTAGACTAGACGCACATCAGGAAGACAGTTAATCCCGTCATGTATTGACCCGGAAACATCATCTCATTTGCCTATAAGACAGAGAGAGAGAATGATAGAAATGCTGGAAGAACGAGCCTTGGGGCGGTATCGCCCAGATCTGAATCCACACAGCAGCTGCTTCGGTCCCTGTAACTCAAGGCTTCCTCTTAATCCTCTGTCCTCCATCAACACCTGGCTGAGCAGGAACAACTATGACCTAGGACCAGGGCCTTCAGGAATCTTCACCCCTAACTTGAACTTCATAACACTGGAGAGAGAGACTGTGGAATGGAATGACTCTCCAGAGCTGCAGATTGAAGGCATATTTTCATCTGACTTTGATCTCAATGGCGGGAACACCTTCATGGCCCCTTTATAGCTGGTATGTTTTCTTCTTATGGACAATGAGAAACATGTAATAAACTGTGTTTCCTTCTCGCTAGG\n",
            "Metadata: {'Topic': 'Breast cancer'}, Similar Sequence: CTTGCCGCAGCACACTCTCTAATCGAGCTGTAACACTCGCCACTGCCGTCCACGGCTTCATTCCTTGAAGCCGTGAGACCACGAACCCTTCGATTGAGAAGAACCTTCGGTCGGAAGAAGACTTCTCGTCTCAGTAACAATAACAAGAAAAACAGCCTAGACTAGACGCACATCAGGAAGACAGTTAATCCCGTCATGTATTGACCCGGAAACATCATCTCATTTGCCTATAAGACAGAGAGAGAGAATGATAGAAATGCTGGAAGAACGAGCCTTGGGGCGGTATCGCCCAGATCTGAATCCACACAGCAGCTGCTTCGGTCCCTGTAACTCAAGGCTTCCTCTTAATCCTCTGTCCTCCATCAACACCTGGCTGAGCAGGAACAACTATGACCTAGGACCAGGGCCTTCAGGAATCTTCACCCCTAACTTGAACTTCATAACACTGGAGAGAGAGACTGTGGAATGGAATGACTCTCCAGAGCTGCAGATTGAAGGCATATTTTCATCTGACTTTGATCTCAATGGCGGGAACACCTTCATGGCCCCTTTATAGCTGGTATGTTTTCTTCTTATGGACAATGAGAAACATGTAATAAACTGTGTTTCCTTCTCGCTAGG, Similarity Score: 1.00\n",
            "Metadata: {'Topic': 'Non coding variant'}, Similar Sequence: CTTGCCGCAGCACACTCTCTAATCGAGCTGTAACACTCGCCACTGCCGTCCACGGCTTCATTCCTTGAAGCCGTGAGACCACGAACCCTTCGATTGAGAAGAACCTTCGGTCGGAAGAAGACTTCTCGTCTCAGTAACAATAACAAGAAAAACAGCCTAGACTAGACGCACATCAGGAAGACAGTTAATCCCGTCATGTATTGACCCGGAAACATCATCTCATTTGCCTATAAGACAGAGAGAGAGAATGATAGAAATGCTGGAAGAACGAGCCTTGGGGCGGTATCGCCCAGATCTGAATCCACACAGCAGCTGCTTCGGTCCCTGTAACTCAAGGCTTCCTCTTAATCCTCTGTCCTCCATCAACACCTGGCTGAGCAGGAACAACTATGACCTAGGACCAGGGCCTTCAGGAATCTTCACCCCTAACTTGAACTTCATAACACTGGAGAGAGAGACTGTGGAATGGAATGACTCTCCAGAGCTGCAGATTGAAGGCATATTTTCATCTGACTTTGATCTCAATGGCGGGAACACCTTCATGGCCCCTTTATAGCTGGTATGTTTTCTTCTTATGGACAATGAGAAACATGTAATAAACTGTGTTTCCTTCTCGCTAGG, Similarity Score: 1.00\n",
            "Metadata: {'Topic': 'Breast cancer'}, Similar Sequence: AGTTAGTGCTGGGAAACAGTGCTAAGAAGGATACAGTGGCTAGAAGTCGTCCTGTCGTCCTGCCTCACAGTAACATCGTTACCGAATTCTCAGCAGGTGAACCAAATGAAATGGTCAACTGAAAGCCAACCAGGGGTCTTGTCCTGTCACTCTGGCTGGAATACAATGGCGTAATCATAGCTCACTGCGGCCTCCATCTCCTGGGTTCAAGTGATTCTCCTGCTTCAGCTTCCCAAGTATCTGGGACTACAGCAAAAAATCACCATGTACCAACCTATCCAAACTTATCCATGGATGAATCTATCCAGAAGACGGGAGTTCCGATGCTTGTCTTGCTCTGAATGTCTGCTTGTCACCTGCTTAGGGTTATCGACTGTGATTCTGGGACTCATTGTTGTTCTACAGGACCCCTCTGACTCTGTGGTTTTCTCTACTGGATTAACAATGATAGCCATAGGTGCTTTTTTTGTTGTCCTCACTGGAGTGACAGCCCTGTGTACGGTTACAGTCGACGAGAACTTGCAGAAAACCACGAGGCTAAGACTAGGAGTGATACGAAAAAGCGGAAGTCTCCAAGGAACTACAGAGCCTTCCATGACTCACTCAATAATCGCTAGCACCTCGCTGTAGTTGTACATTGAACCCTGGCATCTTCGTCTTTGGAACTAAGTCTCCTGAGCATTGTTTTTAAATAGAAATAAAATCTGGCTTTTAAAAAAAAAAAAAAA, Similarity Score: 0.32\n",
            "\n",
            "Query Sequence: AGTTAGTGCTGGGAAACAGTGCTAAGAAGGATACAGTGGCTAGAAGTCGTCCTGTCGTCCTGCCTCACAGTAACATCGTTACCGAATTCTCAGCAGGTGAACCAAATGAAATGGTCAACTGAAAGCCAACCAGGCAAAAAATCACCATGTACCAACCTATCCAAACTTATCCATGGATGAATCTATCCAGAAGACGGGAGTTCCGATGCTTGTCTTGCTCTGAATGTCTGCTTGTCACCTGCTTAGGGTTATCGACTGTGATTCTGGGACTCATTGTTGTTCTACAGGACCCCTCTGACTCTGTGGTTTTCTCTACTGGATTAACAATGATAGCCATAGGTGCTTTTTTTGTTGTCCTCACTGGAGTGACAGCCCTGTGTACGGTTACAGTCGACGAGAACTTGCAGAAAACCACGAGGCTAAGACTAGGAGTGATACGAAAAAGCGGAAGTCTCCAAGGAACTACAGAGCCTTCCATGACTCACTCAATAATCGCTAGCACCTCGCTGTAGTTGTACATTGAACCCTGGCATCTTCGTCTTTGGAACTAAGTCTCCTGAGCATTGTTTTTAAATAGAAATAAAATCTGGCTTTTAAAAAAAAAAAAAAAD\n",
            "Metadata: {'Topic': 'Breast cancer'}, Similar Sequence: AGTTAGTGCTGGGAAACAGTGCTAAGAAGGATACAGTGGCTAGAAGTCGTCCTGTCGTCCTGCCTCACAGTAACATCGTTACCGAATTCTCAGCAGGTGAACCAAATGAAATGGTCAACTGAAAGCCAACCAGGCAAAAAATCACCATGTACCAACCTATCCAAACTTATCCATGGATGAATCTATCCAGAAGACGGGAGTTCCGATGCTTGTCTTGCTCTGAATGTCTGCTTGTCACCTGCTTAGGGTTATCGACTGTGATTCTGGGACTCATTGTTGTTCTACAGGACCCCTCTGACTCTGTGGTTTTCTCTACTGGATTAACAATGATAGCCATAGGTGCTTTTTTTGTTGTCCTCACTGGAGTGACAGCCCTGTGTACGGTTACAGTCGACGAGAACTTGCAGAAAACCACGAGGCTAAGACTAGGAGTGATACGAAAAAGCGGAAGTCTCCAAGGAACTACAGAGCCTTCCATGACTCACTCAATAATCGCTAGCACCTCGCTGTAGTTGTACATTGAACCCTGGCATCTTCGTCTTTGGAACTAAGTCTCCTGAGCATTGTTTTTAAATAGAAATAAAATCTGGCTTTTAAAAAAAAAAAAAAA, Similarity Score: 0.98\n",
            "Metadata: {'Topic': 'Breast cancer'}, Similar Sequence: AGTTAGTGCTGGGAAACAGTGCTAAGAAGGATACAGTGGCTAGAAGTCGTCCTGTCGTCCTGCCTCACAGTAACATCGTTACCGAATTCTCAGCAGGTGAACCAAATGAAATGGTCAACTGAAAGCCAACCAGGAGGGTGTCTGGCTGTTTTGGGGAAACTATTCCTGACCTTATTTTGACTAAAAAGTTGCCTGCTGTACCAGCAAAAAATCACCATGTACCAACCTATCCAAACTTATCCATGGATGAATCTATCCAGAAGACGGGAGTTCCGATGCTTGTCTTGCTCTGAATGTCTGCTTGTCACCTGCTTAGGGTTATCGACTGTGATTCTGGGACTCATTGTTGTTCTACAGGACCCCTCTGACTCTGTGGTTTTCTCTACTGGATTAACAATGATAGCCATAGGTGCTTTTTTTGTTGTCCTCACTGGAGTGACAGCCCTGTGTACGGTTACAGTCGACGAGAACTTGCAGAAAACCACGAGGCTAAGACTAGGAGTGATACGAAAAAGCGGAAGTCTCCAAGGAACTACAGAGCCTTCCATGACTCACTCAATAATCGCTAGCACCTCGCTGTAGTTGTACATTGAACCCTGGCATCTTCGTCTTTGGAACTAAGTCTCCTGAGCATTGTTTTTAAATAGAAATAAAATCTGGCTTTTAAAAAAAAAAAAAAA, Similarity Score: 0.90\n",
            "Metadata: {'Topic': 'Breast cancer'}, Similar Sequence: AGTTAGTGCTGGGAAACAGTGCTAAGAAGGATACAGTGGCTAGAAGTCGTCCTGTCGTCCTGCCTCACAGTAACATCGTTACCGAATTCTCAGCAGGTGAACCAAATGAAATGGTCAACTGAAAGCCAACCAGGTCTCTCCTTGTGAGAAGTGAATTTCTTCAACGTTTATAGAACTGAGGTATTACATTATTGGATGAATTAAGAAAACAATCTAACCTGATGTGTGAAAATTTCTGCTTGTGAGAATCCGTGTTATTTCAATTATCCAATCAAGAGCCTAATTCGTATAAAAGAAACACAGCAGCTTGTTGCTCATCTTTTTATCTAAGGACGGTTTGTCTTGACAGAGGGTGTCTGGCTGTTTTGGGGAAACTATTCCTGACCTTATTTTGACTAAAAAGTTGCCTGCTGTACCAGCAAAAAATCACCATGTACCAACCTATCCAAACTTATCCATGGATGAATCTATCCAGAAGACGGGAGTTCCGATGCTTGTCTTGCTCTGAATGTCTGCTTGTCACCTGCTTAGGGTTATCGACTGTGATTCTGGGACTCATTGTTGTTCTACAGGACCCCTCTGACTCTGTGGTTTTCTCTACTGGATTAACAATGATAGCCATAGGTGCTTTTTTTGTTGTCCTCACTGGAGTGACAGCCCTGTGTACGGTTACAGTCGACGAGAACTTGCAGAAAACCACGAGGCTAAGACTAGGAGTGATACGAAAAAGCGGAAGTCTCCAAGGAACTACAGAGCCTTCCATGACTCACTCAATAATCGCTAGCACCTCGCTGTAGTTGTACATTGAACCCTGGCATCTTCGTCTTTGGAACTAAGTCTCCTGAGCATTGTTTTTAAATAGAAATAAAATCTGGCTTTTAAAAAAAAAAAAAAA, Similarity Score: 0.81\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Update entry**"
      ],
      "metadata": {
        "id": "Ep5G_EIB1F2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python seq_vdb.py update 5 --new_metadata '{\"Topic\": \"query\"}' /content/query.fasta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp8xq3QObdR9",
        "outputId": "8a2ff063-13dd-42cb-d94c-0182953337e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n",
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Database loaded from disk.\n",
            "Database saved to disk.\n",
            "Inserted 2 sequences starting at position 5.\n"
          ]
        }
      ]
    }
  ]
}